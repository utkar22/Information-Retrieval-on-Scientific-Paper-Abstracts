{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/utkar22/CSE508_Winter2023_A1_48/blob/main/Using_Positional_Index.ipynb",
      "authorship_tag": "ABX9TyMu6W9T+RYQmUyyJYg39+zX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkar22/CSE508_Winter2023_A1_48/blob/main/Using_Positional_Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrB57vUVRPXP",
        "outputId": "9da6de2e-b33b-4aae-c947-4852c39bd113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from typing import List\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "class preprocessor:\n",
        "  def __init__(self):\n",
        "    self.value = 1\n",
        "  \n",
        "  def read_file(self, path:str, type_of:str) -> str:\n",
        "\n",
        "    '''reads the contents of a file and converts them into \n",
        "    a string of words separated by space.'''\n",
        "    f = open(path, 'r')\n",
        "    text = ''\n",
        "    words = []\n",
        "    for line in f:\n",
        "      for word in line.split(\" \"):\n",
        "        text += word + ' '\n",
        "        words.append(word)\n",
        "    f.close()\n",
        "    return text if type_of == 'text' else words\n",
        "  \n",
        "  def tokenize(self, text:str):\n",
        "    ''' The function takes in a string and \n",
        "    converts it into a list of tokens using nltk'''\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "  def tokens_to_text(self, tokens:List) -> str:\n",
        "    ''' converts the tokens into space-separated string'''\n",
        "    text = ''\n",
        "    for token in tokens:\n",
        "\n",
        "      text += '' + token + ' '\n",
        "    return text\n",
        "\n",
        "  def lowercase(self, tokens:List):\n",
        "    ''' This function takes in the list of tokens\n",
        "     and returns the lowercase version of each token without changing the index positions'''\n",
        "\n",
        "    for i in range(len(tokens)):\n",
        "      tokens[i] = tokens[i].lower()\n",
        "    return tokens\n",
        "\n",
        "  def remove_stopwords(self, tokens:List) -> List:\n",
        "    ''' removes any stop words present as tokens in the list of tokens \n",
        "    present using nltk's corpus of stopwords'''\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    resultant_tokens = []\n",
        "    for token in tokens:\n",
        "      if token not in stop_words:\n",
        "        resultant_tokens.append(token)\n",
        "    return resultant_tokens\n",
        "\n",
        "  def remove_puncts(self, text:str) -> List:\n",
        "    ''' removes any punctuations present as tokens \n",
        "    and returns the resutlant list of tokens'''\n",
        "\n",
        "    resultant_tokens = []\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    resultant_tokens = self.tokenize(text)\n",
        "    return resultant_tokens\n",
        "\n",
        "  def write_to_file(self, path:str, text:str) ->None:\n",
        "    '''wrtie the text to the file '''\n",
        "    open(path, 'w').close()\n",
        "    f = open(path, 'w')\n",
        "    f.write(text)\n",
        "    f.close()\n",
        "\n",
        "  def make_doc_id_mapping(self, path_to_folder:str)->dict:\n",
        "    '''make the mapping of doc_id : doc'''\n",
        "    mapping = {}\n",
        "    folder = os.fsencode(path_to_folder)\n",
        "    for doc in os.listdir(folder):\n",
        "      doc_name = os.fsdecode(doc)\n",
        "      if doc_name.find('cranfield') == -1:\n",
        "        continue\n",
        "      number = (int)(doc_name[-4:])\n",
        "      mapping[number] = doc_name\n",
        "    return mapping\n",
        "\n"
      ],
      "metadata": {
        "id": "MNEqEXEBgZMi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class word:\n",
        "  ''' Contains the {count of documents which contain the word, a map of doc_id to the positions in that doc}'''\n",
        "  def __init__(self):\n",
        "    self.value = None\n",
        "    self.freq = 1\n",
        "    self.docs = {}\n",
        "\n",
        "class doc:\n",
        "  def __init__(self, did):\n",
        "    self.docId = did\n",
        "    self.positions = []\n",
        "dictionary = {}"
      ],
      "metadata": {
        "id": "7FFdsLGKSYkU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_id_mapping = {}\n",
        "with open('/content/drive/MyDrive/IR_Assignments/A1/doc_id_mapping.pkl', 'rb') as f:\n",
        "  doc_id_mapping = pickle.load(f)\n",
        "print(len(doc_id_mapping))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-3Gv2c9H5QS",
        "outputId": "a5c55c55-e26e-4730-ae30-cf35da638cd9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = {}\n",
        "with open('/content/drive/MyDrive/IR_Assignments/A1/positional_index.pkl', 'rb') as f:\n",
        "  dictionary = pickle.load(f)\n",
        "print(len(dictionary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzLHtYFxR3NR",
        "outputId": "cd64ce6e-814d-4038-e288-52577812d75c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proc = preprocessor()\n",
        "def preprocess_query(phrase:str)->List:\n",
        "  phrase_tokens = proc.tokenize(phrase)\n",
        "  phrase_tokens = proc.lowercase(phrase_tokens)\n",
        "  phrase_tokens = proc.remove_stopwords(phrase_tokens)\n",
        "  phrase_tokens = proc.remove_puncts(proc.tokens_to_text(phrase_tokens))\n",
        "  return phrase_tokens\n",
        "\n",
        "def intersect_docs(phrase:str, dictionary:dict, cands:set):\n",
        "  phrase_tokens = preprocess_query(phrase)\n",
        "  for token in phrase_tokens:\n",
        "    if token in dictionary.keys():\n",
        "      cands = cands & set(dictionary[token].docs) # intersection of doc lists where all words of a phrase are present\n",
        "    else:\n",
        "      return {}\n",
        "  return cands\n",
        "\n",
        "phrase = 'study'\n",
        "candidates = set(list(range(1, len(dictionary)+1)))\n",
        "print(len(candidates))"
      ],
      "metadata": {
        "id": "45kw9k3Cqx24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518c26bb-8f38-4f42-9ad6-e1179b0f456b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process(phrase:str)->List:\n",
        "  res_list = []\n",
        "  candidate_docs = candidates\n",
        "\n",
        "  tokens = preprocess_query(phrase) #preprocess the query\n",
        "  if len(tokens) == 0:\n",
        "    return []\n",
        "  candidate_docs = intersect_docs(phrase, dictionary, candidate_docs) #find out the candidate doc_ids as the intersection of all word postings within a phrase\n",
        "  \n",
        "  if len(candidate_docs) == 0:\n",
        "    return []\n",
        "  for doc_id in candidate_docs: #check each doc_id\n",
        "\n",
        "    inter_list = dictionary[tokens[0]].docs[doc_id].positions # go to word -> doc_id -> postions list\n",
        "\n",
        "    for idx in range(1, len(tokens)):\n",
        "\n",
        "      pos_list = dictionary[tokens[idx]].docs[doc_id].positions #get the positions of the current word\n",
        "      inter_list_copy = set([x+1 for x in inter_list]) \n",
        "      inter_list =  list(inter_list_copy & set(pos_list)) #check for intersection betweein phrase[0:i] and phrase[i]\n",
        "\n",
        "    if len(inter_list) == 0: #if no intersection found do not add\n",
        "      continue\n",
        "    else:\n",
        "      res_list.append(doc_id)\n",
        "\n",
        "  return sorted(res_list)"
      ],
      "metadata": {
        "id": "Xlee-7LjNFgi"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GjM7PdWeb-Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mapping command to function\n",
        "\n",
        "def solve(list1, list2):\n",
        "    return AND(list1,list2)"
      ],
      "metadata": {
        "id": "tdTW_d7b_rlH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_file='/content/drive/MyDrive/IR_Assignments/A1/BIGRAM.pkl'\n",
        "with open(source_file, 'rb') as f:\n",
        "    BIGRAM=pickle.load(f)\n",
        "doc_mapping  = {}\n",
        "with open('/content/drive/MyDrive/IR_Assignments/A1/doc_id_mapping.pkl', 'rb') as f:\n",
        "  doc_mapping = pickle.load(f)"
      ],
      "metadata": {
        "id": "hb3bifC-cT7a"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solving generalised query\n",
        "\n",
        "def compute(words, commands):\n",
        "  sol=[]\n",
        "  total=0\n",
        "  query=''\n",
        "  t1=words[0]\n",
        "  t2=words[1]\n",
        "\n",
        "  if(t1 not in BIGRAM):\n",
        "    list1=[]\n",
        "  else:\n",
        "    list1=BIGRAM[t1]\n",
        "  \n",
        "  if(t2 not in BIGRAM):\n",
        "    list2=[]\n",
        "  else:\n",
        "    list2=BIGRAM[t2]\n",
        "  \n",
        "  sol,comp=solve(list1,list2)\n",
        "  query+=t1+' '+commands[0]+' '+t2\n",
        "\n",
        "      \n",
        "  total+=comp\n",
        "\n",
        "  for i in range(2,len(words)):\n",
        "\n",
        "    t=words[i]\n",
        "\n",
        "    if(t not in BIGRAM):\n",
        "      lst=[]\n",
        "    else:\n",
        "      lst=BIGRAM[t]\n",
        "    \n",
        "    sol,comp=solve(sol,lst,commands[i-1])\n",
        "    \n",
        "    total+=comp\n",
        "    query+=' '+commands[i-1]+' '+t\n",
        "\n",
        "  return query, sol, total  "
      ],
      "metadata": {
        "id": "z6DuYv0E5QVF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AND(list1, list2):\n",
        "  comp=0\n",
        "\n",
        "  if list1==[] or list2==[]:\n",
        "    return [],comp\n",
        "\n",
        "  sol=[]\n",
        "\n",
        "  idx1=0\n",
        "  idx2=0\n",
        "\n",
        "  while idx1<len(list1)  and idx2< len(list2):\n",
        "    comp+=1\n",
        "\n",
        "    if list1[idx1]==list2[idx2]:\n",
        "      sol.append(list1[idx1])\n",
        "      idx1+=1\n",
        "      idx2+=1\n",
        "    \n",
        "    elif list1[idx1]<list2[idx2]:\n",
        "      idx1+=1\n",
        "    \n",
        "    else:\n",
        "      idx2+=1\n",
        "\n",
        "  return sol,comp\n"
      ],
      "metadata": {
        "id": "OQDTPpW3zu1I"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solving preprocessed query\n",
        "\n",
        "def simpler_query(tokens):\n",
        "\n",
        "  words_lst=tokens\n",
        "\n",
        "\n",
        "  words=[words_lst[i]+' '+words_lst[i+1] for i in range(len(words_lst)-1) if (words_lst[i].strip()!='' and words_lst[i+1].strip()!='')]\n",
        "  # print(words)\n",
        "\n",
        "  commands=['AND' for i in range(len(words)-1)]\n",
        "\n",
        "  return compute(words,commands)"
      ],
      "metadata": {
        "id": "BgIP4HqbABOk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to process query\n",
        "\n",
        "def query(no, inp):\n",
        "\n",
        "  tokens=preprocess_query(inp)\n",
        "  if len(tokens) == 0:\n",
        "    print(f'Number of documents retrieved for query {no} using Bigram Inverted Index : 0')\n",
        "    print(f'Names of the documents retrieved for query {no} using Bigram Inverted Index :  [] ')\n",
        "    return\n",
        "  query, ans, comparisions= simpler_query(tokens)\n",
        "  docNames=''\n",
        "  for id in ans:\n",
        "    docNames+=doc_mapping[id]+' '\n",
        "\n",
        "  \n",
        "  print('Number of documents retrieved for query ' +str(no)+' using Bigram Inverted Index : '+ str(len(ans)))\n",
        "  print('Names of the documents retrieved for query '+ str(no)+ ' using Bigram Inverted Index : '+ docNames)\n"
      ],
      "metadata": {
        "id": "ulEMk2EmBQln"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_query():\n",
        "  t = int(input(\"Enter the number of queries: \"))\n",
        "  idx = 1\n",
        "  while t > 0:\n",
        "    phrase = input(\"Enter the phrase: \")\n",
        "    res_list_using_pi = process(phrase)\n",
        "    res_list_using_bi = query(idx, phrase)\n",
        "\n",
        "    print(f'Number of documents retrieved for query {idx} using positional index: {len(res_list_using_pi)}')\n",
        "    print(f'Names of documents retrieved for query {idx} using positional index: ', end = \" \")\n",
        "    for doc_id in res_list_using_pi:\n",
        "      print(doc_id_mapping[doc_id], end=\", \")\n",
        "    print()\n",
        "    idx+=1\n",
        "    t-=1\n",
        "run_query()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YENV1KP-HBX3",
        "outputId": "2b8bcc1f-bad0-4168-bf9c-5bb7c9fd9f42"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 3\n",
            "Enter the phrase: this is it\n",
            "Number of documents retrieved for query 1 using Bigram Inverted Index : 0\n",
            "Names of the documents retrieved for query 1 using Bigram Inverted Index :  [] \n",
            "Number of documents retrieved for query 1 using positional index: 0\n",
            "Names of documents retrieved for query 1 using positional index:  \n",
            "Enter the phrase: velocity\n",
            "[]\n",
            "Number of documents retrieved for query 2 using Bigram Inverted Index : 0\n",
            "Names of the documents retrieved for query 2 using Bigram Inverted Index : \n",
            "Number of documents retrieved for query 2 using positional index: 278\n",
            "Names of documents retrieved for query 2 using positional index:  cranfield0001, cranfield0004, cranfield0007, cranfield0014, cranfield0015, cranfield0018, cranfield0035, cranfield0036, cranfield0043, cranfield0044, cranfield0049, cranfield0050, cranfield0051, cranfield0054, cranfield0059, cranfield0060, cranfield0061, cranfield0062, cranfield0070, cranfield0071, cranfield0072, cranfield0073, cranfield0074, cranfield0077, cranfield0081, cranfield0087, cranfield0088, cranfield0094, cranfield0097, cranfield0099, cranfield0101, cranfield0105, cranfield0106, cranfield0108, cranfield0112, cranfield0115, cranfield0116, cranfield0126, cranfield0128, cranfield0131, cranfield0132, cranfield0133, cranfield0138, cranfield0139, cranfield0145, cranfield0148, cranfield0151, cranfield0154, cranfield0155, cranfield0156, cranfield0157, cranfield0162, cranfield0163, cranfield0164, cranfield0165, cranfield0166, cranfield0169, cranfield0177, cranfield0178, cranfield0183, cranfield0191, cranfield0192, cranfield0194, cranfield0197, cranfield0203, cranfield0206, cranfield0207, cranfield0210, cranfield0212, cranfield0213, cranfield0215, cranfield0216, cranfield0217, cranfield0218, cranfield0219, cranfield0224, cranfield0236, cranfield0240, cranfield0241, cranfield0243, cranfield0247, cranfield0249, cranfield0255, cranfield0257, cranfield0260, cranfield0261, cranfield0263, cranfield0267, cranfield0268, cranfield0269, cranfield0275, cranfield0277, cranfield0278, cranfield0290, cranfield0292, cranfield0296, cranfield0299, cranfield0304, cranfield0305, cranfield0308, cranfield0321, cranfield0325, cranfield0327, cranfield0328, cranfield0339, cranfield0341, cranfield0344, cranfield0351, cranfield0352, cranfield0364, cranfield0369, cranfield0378, cranfield0381, cranfield0383, cranfield0386, cranfield0397, cranfield0401, cranfield0406, cranfield0407, cranfield0414, cranfield0415, cranfield0417, cranfield0426, cranfield0447, cranfield0452, cranfield0453, cranfield0457, cranfield0458, cranfield0459, cranfield0460, cranfield0468, cranfield0474, cranfield0475, cranfield0477, cranfield0478, cranfield0481, cranfield0484, cranfield0491, cranfield0495, cranfield0504, cranfield0512, cranfield0515, cranfield0524, cranfield0534, cranfield0538, cranfield0549, cranfield0550, cranfield0556, cranfield0557, cranfield0559, cranfield0562, cranfield0569, cranfield0608, cranfield0611, cranfield0646, cranfield0651, cranfield0653, cranfield0654, cranfield0663, cranfield0681, cranfield0682, cranfield0689, cranfield0695, cranfield0701, cranfield0717, cranfield0721, cranfield0750, cranfield0759, cranfield0770, cranfield0771, cranfield0773, cranfield0774, cranfield0779, cranfield0787, cranfield0788, cranfield0790, cranfield0812, cranfield0813, cranfield0814, cranfield0872, cranfield0880, cranfield0882, cranfield0894, cranfield0899, cranfield0900, cranfield0905, cranfield0906, cranfield0918, cranfield0919, cranfield0940, cranfield0943, cranfield0944, cranfield0949, cranfield0959, cranfield0960, cranfield0961, cranfield0962, cranfield0963, cranfield0965, cranfield0966, cranfield0967, cranfield0977, cranfield0984, cranfield0985, cranfield0986, cranfield0987, cranfield0989, cranfield0990, cranfield1010, cranfield1061, cranfield1064, cranfield1072, cranfield1076, cranfield1080, cranfield1082, cranfield1109, cranfield1139, cranfield1141, cranfield1149, cranfield1151, cranfield1155, cranfield1157, cranfield1161, cranfield1182, cranfield1197, cranfield1199, cranfield1204, cranfield1208, cranfield1209, cranfield1210, cranfield1215, cranfield1216, cranfield1220, cranfield1222, cranfield1224, cranfield1235, cranfield1238, cranfield1240, cranfield1241, cranfield1242, cranfield1250, cranfield1251, cranfield1254, cranfield1260, cranfield1261, cranfield1268, cranfield1273, cranfield1274, cranfield1275, cranfield1278, cranfield1282, cranfield1283, cranfield1291, cranfield1301, cranfield1302, cranfield1303, cranfield1309, cranfield1319, cranfield1321, cranfield1327, cranfield1330, cranfield1335, cranfield1344, cranfield1345, cranfield1347, cranfield1348, cranfield1352, cranfield1366, cranfield1368, cranfield1370, cranfield1371, cranfield1372, cranfield1374, cranfield1375, cranfield1382, cranfield1383, cranfield1386, cranfield1393, \n",
            "Enter the phrase: aerodynamic\n",
            "[]\n",
            "Number of documents retrieved for query 3 using Bigram Inverted Index : 0\n",
            "Names of the documents retrieved for query 3 using Bigram Inverted Index : \n",
            "Number of documents retrieved for query 3 using positional index: 157\n",
            "Names of documents retrieved for query 3 using positional index:  cranfield0005, cranfield0013, cranfield0014, cranfield0029, cranfield0032, cranfield0036, cranfield0044, cranfield0051, cranfield0052, cranfield0066, cranfield0073, cranfield0077, cranfield0095, cranfield0120, cranfield0129, cranfield0137, cranfield0141, cranfield0142, cranfield0163, cranfield0164, cranfield0172, cranfield0185, cranfield0202, cranfield0203, cranfield0204, cranfield0205, cranfield0225, cranfield0272, cranfield0277, cranfield0287, cranfield0329, cranfield0337, cranfield0342, cranfield0379, cranfield0390, cranfield0391, cranfield0406, cranfield0415, cranfield0434, cranfield0441, cranfield0442, cranfield0452, cranfield0453, cranfield0464, cranfield0481, cranfield0486, cranfield0499, cranfield0530, cranfield0536, cranfield0544, cranfield0546, cranfield0567, cranfield0592, cranfield0598, cranfield0599, cranfield0606, cranfield0608, cranfield0625, cranfield0627, cranfield0632, cranfield0635, cranfield0638, cranfield0650, cranfield0658, cranfield0671, cranfield0685, cranfield0688, cranfield0689, cranfield0698, cranfield0704, cranfield0707, cranfield0708, cranfield0709, cranfield0711, cranfield0712, cranfield0715, cranfield0716, cranfield0717, cranfield0719, cranfield0746, cranfield0748, cranfield0749, cranfield0753, cranfield0759, cranfield0780, cranfield0781, cranfield0783, cranfield0794, cranfield0798, cranfield0801, cranfield0812, cranfield0813, cranfield0814, cranfield0815, cranfield0859, cranfield0860, cranfield0861, cranfield0877, cranfield0886, cranfield0892, cranfield0894, cranfield0896, cranfield0899, cranfield0917, cranfield0919, cranfield0925, cranfield0927, cranfield0939, cranfield0947, cranfield0972, cranfield0978, cranfield0981, cranfield0982, cranfield0999, cranfield1005, cranfield1008, cranfield1064, cranfield1066, cranfield1089, cranfield1104, cranfield1112, cranfield1115, cranfield1147, cranfield1156, cranfield1162, cranfield1163, cranfield1164, cranfield1195, cranfield1197, cranfield1209, cranfield1244, cranfield1246, cranfield1259, cranfield1272, cranfield1274, cranfield1289, cranfield1291, cranfield1305, cranfield1314, cranfield1319, cranfield1320, cranfield1328, cranfield1332, cranfield1333, cranfield1334, cranfield1335, cranfield1336, cranfield1339, cranfield1340, cranfield1342, cranfield1343, cranfield1345, cranfield1347, cranfield1352, cranfield1379, cranfield1380, cranfield1391, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-_dxeuJvdeg6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LoFqt6xdfGq",
        "outputId": "6a80b3cc-da0b-4efc-8a32-93af0c0aa711"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "this is in\n",
            "Number of documents retrieved for query 1 using Bigram Inverted Index : 0\n",
            "Names of the documents retrieved for query 1 using Bigram Inverted Index :  [] \n",
            "this is it\n",
            "Number of documents retrieved for query 2 using Bigram Inverted Index : 0\n",
            "Names of the documents retrieved for query 2 using Bigram Inverted Index :  [] \n"
          ]
        }
      ]
    }
  ]
}