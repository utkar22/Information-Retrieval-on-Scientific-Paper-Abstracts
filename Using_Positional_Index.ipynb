{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19TnVx9zkxiCW_cBIg_w0duwd24Z50u6m",
      "authorship_tag": "ABX9TyPlakAYX8/NkI9OvcL93koJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkar22/CSE508_Winter2023_A1_48/blob/main/Using_Positional_Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrB57vUVRPXP",
        "outputId": "e6fe782a-dfa4-4745-8d1a-a0ce6294a804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from typing import List\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "class preprocessor:\n",
        "  def __init__(self):\n",
        "    self.value = 1\n",
        "  \n",
        "  def read_file(self, path:str, type_of:str) -> str:\n",
        "\n",
        "    '''reads the contents of a file and converts them into \n",
        "    a string of words separated by space.'''\n",
        "    f = open(path, 'r')\n",
        "    text = ''\n",
        "    words = []\n",
        "    for line in f:\n",
        "      for word in line.split(\" \"):\n",
        "        text += word + ' '\n",
        "        words.append(word)\n",
        "    f.close()\n",
        "    return text if type_of == 'text' else words\n",
        "  \n",
        "  def tokenize(self, text:str):\n",
        "    ''' The function takes in a string and \n",
        "    converts it into a list of tokens using nltk'''\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "  def tokens_to_text(self, tokens:List) -> str:\n",
        "    ''' converts the tokens into space-separated string'''\n",
        "    text = ''\n",
        "    for token in tokens:\n",
        "\n",
        "      text += '' + token + ' '\n",
        "    return text\n",
        "\n",
        "  def lowercase(self, tokens:List):\n",
        "    ''' This function takes in the list of tokens\n",
        "     and returns the lowercase version of each token without changing the index positions'''\n",
        "\n",
        "    for i in range(len(tokens)):\n",
        "      tokens[i] = tokens[i].lower()\n",
        "    return tokens\n",
        "\n",
        "  def remove_stopwords(self, tokens:List) -> List:\n",
        "    ''' removes any stop words present as tokens in the list of tokens \n",
        "    present using nltk's corpus of stopwords'''\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    resultant_tokens = []\n",
        "    for token in tokens:\n",
        "      if token not in stop_words:\n",
        "        resultant_tokens.append(token)\n",
        "    return resultant_tokens\n",
        "\n",
        "  def remove_puncts(self, text:str) -> List:\n",
        "    ''' removes any punctuations present as tokens \n",
        "    and returns the resutlant list of tokens'''\n",
        "\n",
        "    resultant_tokens = []\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    resultant_tokens = self.tokenize(text)\n",
        "    return resultant_tokens\n",
        "\n",
        "  def write_to_file(self, path:str, text:str) ->None:\n",
        "    '''wrtie the text to the file '''\n",
        "    open(path, 'w').close()\n",
        "    f = open(path, 'w')\n",
        "    f.write(text)\n",
        "    f.close()\n",
        "\n",
        "  def make_doc_id_mapping(self, path_to_folder:str)->dict:\n",
        "    '''make the mapping of doc_id : doc'''\n",
        "    mapping = {}\n",
        "    folder = os.fsencode(path_to_folder)\n",
        "    for doc in os.listdir(folder):\n",
        "      doc_name = os.fsdecode(doc)\n",
        "      if doc_name.find('cranfield') == -1:\n",
        "        continue\n",
        "      number = (int)(doc_name[-4:])\n",
        "      mapping[number] = doc_name\n",
        "    return mapping\n",
        "\n"
      ],
      "metadata": {
        "id": "MNEqEXEBgZMi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class word:\n",
        "  ''' Contains the {count of documents which contain the word, a map of doc_id to the positions in that doc}'''\n",
        "  def __init__(self):\n",
        "    self.value = None\n",
        "    self.freq = 1\n",
        "    self.docs = {}\n",
        "\n",
        "class doc:\n",
        "  def __init__(self, did):\n",
        "    self.docId = did\n",
        "    self.positions = []\n",
        "dictionary = {}"
      ],
      "metadata": {
        "id": "7FFdsLGKSYkU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_id_mapping = {}\n",
        "with open('/content/drive/MyDrive/IR_Assignments/A1/doc_id_mapping.pkl', 'rb') as f:\n",
        "  doc_id_mapping = pickle.load(f)\n",
        "print(len(doc_id_mapping))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-3Gv2c9H5QS",
        "outputId": "88b68098-f7d6-42c3-dc9b-a0739a3e758f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = {}\n",
        "with open('/content/drive/MyDrive/IR_Assignments/A1/positional_index.pkl', 'rb') as f:\n",
        "  dictionary = pickle.load(f)\n",
        "print(len(dictionary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzLHtYFxR3NR",
        "outputId": "c51f7f60-a9cb-4704-95d5-5276b823a1f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proc = preprocessor()\n",
        "def preprocess_query(phrase:str)->List:\n",
        "  phrase_tokens = proc.tokenize(phrase)\n",
        "  phrase_tokens = proc.lowercase(phrase_tokens)\n",
        "  phrase_tokens = proc.remove_stopwords(phrase_tokens)\n",
        "  phrase_tokens = proc.remove_puncts(proc.tokens_to_text(phrase_tokens))\n",
        "  return phrase_tokens\n",
        "\n",
        "def intersect_docs(phrase:str, dictionary:dict, cands:set):\n",
        "  phrase_tokens = preprocess_query(phrase)\n",
        "  for token in phrase_tokens:\n",
        "    if token in dictionary.keys():\n",
        "      cands = cands & set(dictionary[token].docs) # intersection of doc lists where all words of a phrase are present\n",
        "    else:\n",
        "      return {}\n",
        "  return cands\n",
        "\n",
        "phrase = 'study'\n",
        "candidates = set(list(range(1, len(dictionary)+1)))\n",
        "print(len(candidates))"
      ],
      "metadata": {
        "id": "45kw9k3Cqx24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e296eb5-51fb-44dc-9675-a39bbc614b82"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process(phrase:str)->List:\n",
        "  res_list = []\n",
        "  candidate_docs = candidates\n",
        "\n",
        "  tokens = preprocess_query(phrase) #preprocess the query\n",
        "  if len(tokens) == 0:\n",
        "    return []\n",
        "  candidate_docs = intersect_docs(phrase, dictionary, candidate_docs) #find out the candidate doc_ids as the intersection of all word postings within a phrase\n",
        "  \n",
        "  if len(candidate_docs) == 0:\n",
        "    return []\n",
        "  for doc_id in candidate_docs: #check each doc_id\n",
        "\n",
        "    inter_list = dictionary[tokens[0]].docs[doc_id].positions # go to word -> doc_id -> postions list\n",
        "\n",
        "    for idx in range(1, len(tokens)):\n",
        "\n",
        "      pos_list = dictionary[tokens[idx]].docs[doc_id].positions #get the positions of the current word\n",
        "      inter_list_copy = set([x+1 for x in inter_list]) \n",
        "      inter_list =  list(inter_list_copy & set(pos_list)) #check for intersection betweein phrase[0:i] and phrase[i]\n",
        "\n",
        "    if len(inter_list) == 0: #if no intersection found do not add\n",
        "      continue\n",
        "    else:\n",
        "      res_list.append(doc_id)\n",
        "\n",
        "  return sorted(res_list)"
      ],
      "metadata": {
        "id": "Xlee-7LjNFgi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_query():\n",
        "  t = int(input(\"Enter the number of queries: \"))\n",
        "  idx = 1\n",
        "  while t > 0:\n",
        "    phrase = input(\"Enter the phrase: \")\n",
        "    res_list = process(phrase)\n",
        "    print(f'Number of documents retrieved for query {idx} using positional index: {len(res_list)}')\n",
        "    print(f'Names of documents retrieved for query {idx} using positional index: ', end = \" \")\n",
        "    for doc_id in res_list:\n",
        "      print(doc_id_mapping[doc_id], end=\", \")\n",
        "    print()\n",
        "    idx+=1\n",
        "    t-=1\n",
        "run_query()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YENV1KP-HBX3",
        "outputId": "e2737f32-53a7-443d-cbf9-241d404bd5ef"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 2\n",
            "Enter the phrase: wing-tail is in \n",
            "Number of documents retrieved for query 1 using positional index: 3\n",
            "Names of documents retrieved for query 1 using positional index:  cranfield0060, cranfield0520, cranfield0924, \n",
            "Enter the phrase: aerodynamics for me\n",
            "Number of documents retrieved for query 2 using positional index: 23\n",
            "Names of documents retrieved for query 2 using positional index:  cranfield0001, cranfield0011, cranfield0216, cranfield0225, cranfield0237, cranfield0244, cranfield0284, cranfield0289, cranfield0296, cranfield0297, cranfield0360, cranfield0453, cranfield0634, cranfield0685, cranfield0689, cranfield0753, cranfield0792, cranfield0902, cranfield1206, cranfield1271, cranfield1331, cranfield1347, cranfield1380, \n"
          ]
        }
      ]
    }
  ]
}