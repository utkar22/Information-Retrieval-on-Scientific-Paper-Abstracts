{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkar22/CSE508_Winter2023_A1_48/blob/main/Using_Positional_Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrB57vUVRPXP",
        "outputId": "54069e0b-018f-49e6-9206-a3f601df6f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from typing import List\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Qh8zXV-zMQrb",
        "outputId": "f0354453-2fc5-4c4c-b21b-80664deea3d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "class preprocessor:\n",
        "  def __init__(self):\n",
        "    self.value = 1\n",
        "  \n",
        "  def read_file(self, path:str, type_of:str) -> str:\n",
        "\n",
        "    '''reads the contents of a file and converts them into \n",
        "    a string of words separated by space.'''\n",
        "    f = open(path, 'r')\n",
        "    text = ''\n",
        "    words = []\n",
        "    for line in f:\n",
        "      for word in line.split(\" \"):\n",
        "        text += word + ' '\n",
        "        words.append(word)\n",
        "    f.close()\n",
        "    return text if type_of == 'text' else words\n",
        "  \n",
        "  def tokenize(self, text:str):\n",
        "    ''' The function takes in a string and \n",
        "    converts it into a list of tokens using nltk'''\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "  def tokens_to_text(self, tokens:List) -> str:\n",
        "    ''' converts the tokens into space-separated string'''\n",
        "    text = ''\n",
        "    for token in tokens:\n",
        "\n",
        "      text += '' + token + ' '\n",
        "    return text\n",
        "\n",
        "  def lowercase(self, tokens:List):\n",
        "    ''' This function takes in the list of tokens\n",
        "     and returns the lowercase version of each token without changing the index positions'''\n",
        "\n",
        "    for i in range(len(tokens)):\n",
        "      tokens[i] = tokens[i].lower()\n",
        "    return tokens\n",
        "\n",
        "  def remove_stopwords(self, tokens:List) -> List:\n",
        "    ''' removes any stop words present as tokens in the list of tokens \n",
        "    present using nltk's corpus of stopwords'''\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    resultant_tokens = []\n",
        "    for token in tokens:\n",
        "      if token not in stop_words:\n",
        "        resultant_tokens.append(token)\n",
        "    return resultant_tokens\n",
        "\n",
        "  def remove_puncts(self, text:str) -> List:\n",
        "    ''' removes any punctuations present as tokens \n",
        "    and returns the resutlant list of tokens'''\n",
        "\n",
        "    resultant_tokens = []\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    resultant_tokens = self.tokenize(text)\n",
        "    return resultant_tokens\n",
        "\n",
        "  def write_to_file(self, path:str, text:str) ->None:\n",
        "    '''wrtie the text to the file '''\n",
        "    open(path, 'w').close()\n",
        "    f = open(path, 'w')\n",
        "    f.write(text)\n",
        "    f.close()\n",
        "\n",
        "  def make_doc_id_mapping(self, path_to_folder:str)->dict:\n",
        "    '''make the mapping of doc_id : doc'''\n",
        "    mapping = {}\n",
        "    folder = os.fsencode(path_to_folder)\n",
        "    for doc in os.listdir(folder):\n",
        "      doc_name = os.fsdecode(doc)\n",
        "      if doc_name.find('cranfield') == -1:\n",
        "        continue\n",
        "      number = (int)(doc_name[-4:])\n",
        "      mapping[number] = doc_name\n",
        "    return mapping\n",
        "\n"
      ],
      "metadata": {
        "id": "MNEqEXEBgZMi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class word:\n",
        "  ''' Contains the {count of documents which contain the word, a map of doc_id to the positions in that doc}'''\n",
        "  def __init__(self):\n",
        "    self.value = None\n",
        "    self.freq = 1\n",
        "    self.docs = {}\n",
        "\n",
        "class doc:\n",
        "  def __init__(self, did):\n",
        "    self.docId = did\n",
        "    self.positions = []\n",
        "dictionary = {}"
      ],
      "metadata": {
        "id": "7FFdsLGKSYkU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_id_mapping = {}\n",
        "with open('/content/drive/MyDrive/IR_Assignments/A1/doc_id_mapping.pkl', 'rb') as f:\n",
        "  doc_id_mapping = pickle.load(f)\n",
        "print(len(doc_id_mapping))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-3Gv2c9H5QS",
        "outputId": "1c9248e2-b5b1-4519-da48-3f9ef396a859"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = {}\n",
        "with open('/content/drive/MyDrive/IR_Assignments/A1/positional_index.pkl', 'rb') as f:\n",
        "  dictionary = pickle.load(f)\n",
        "print(len(dictionary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzLHtYFxR3NR",
        "outputId": "e9d6e30d-2992-4375-c3f6-7933ad57e271"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proc = preprocessor()\n",
        "def preprocess_query(phrase:str)->List:\n",
        "  phrase_tokens = proc.tokenize(phrase)\n",
        "  phrase_tokens = proc.lowercase(phrase_tokens)\n",
        "  phrase_tokens = proc.remove_stopwords(phrase_tokens)\n",
        "  phrase_tokens = proc.remove_puncts(proc.tokens_to_text(phrase_tokens))\n",
        "  return phrase_tokens\n",
        "\n",
        "def intersect_docs(phrase:str, dictionary:dict, cands:set):\n",
        "  phrase_tokens = preprocess_query(phrase)\n",
        "  for token in phrase_tokens:\n",
        "    if token in dictionary.keys():\n",
        "      cands = cands & set(dictionary[token].docs) # intersection of doc lists where all words of a phrase are present\n",
        "    else:\n",
        "      return {}\n",
        "  return cands\n",
        "\n",
        "phrase = 'study'\n",
        "candidates = set(list(range(1, len(dictionary)+1)))\n",
        "print(len(candidates))"
      ],
      "metadata": {
        "id": "45kw9k3Cqx24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ba0e9d-84d4-4d33-a115-6d911fdc2f83"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process(phrase:str)->List:\n",
        "  res_list = []\n",
        "  candidate_docs = candidates\n",
        "\n",
        "  tokens = preprocess_query(phrase) #preprocess the query\n",
        "  if len(tokens) == 0:\n",
        "    return []\n",
        "  candidate_docs = intersect_docs(phrase, dictionary, candidate_docs) #find out the candidate doc_ids as the intersection of all word postings within a phrase\n",
        "  \n",
        "  if len(candidate_docs) == 0:\n",
        "    return []\n",
        "  for doc_id in candidate_docs: #check each doc_id\n",
        "\n",
        "    inter_list = dictionary[tokens[0]].docs[doc_id].positions # go to word -> doc_id -> postions list\n",
        "\n",
        "    for idx in range(1, len(tokens)):\n",
        "\n",
        "      pos_list = dictionary[tokens[idx]].docs[doc_id].positions #get the positions of the current word\n",
        "      inter_list_copy = set([x+1 for x in inter_list]) \n",
        "      inter_list =  list(inter_list_copy & set(pos_list)) #check for intersection betweein phrase[0:i] and phrase[i]\n",
        "\n",
        "    if len(inter_list) == 0: #if no intersection found do not add\n",
        "      continue\n",
        "    else:\n",
        "      res_list.append(doc_id)\n",
        "\n",
        "  return sorted(res_list)"
      ],
      "metadata": {
        "id": "Xlee-7LjNFgi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GjM7PdWeb-Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mapping command to function\n",
        "\n",
        "def solve(list1, list2):\n",
        "    return AND(list1,list2)"
      ],
      "metadata": {
        "id": "tdTW_d7b_rlH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_file='/content/drive/MyDrive/IR_Assignments/A1/BIGRAM.pkl'\n",
        "with open(source_file, 'rb') as f:\n",
        "    BIGRAM=pickle.load(f)\n",
        "doc_mapping  = {}\n",
        "with open('/content/drive/MyDrive/IR_Assignments/A1/doc_id_mapping.pkl', 'rb') as f:\n",
        "  doc_mapping = pickle.load(f)"
      ],
      "metadata": {
        "id": "hb3bifC-cT7a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solving generalised query\n",
        "\n",
        "def compute(words, commands):\n",
        "  sol=[]\n",
        "  total=0\n",
        "  query=''\n",
        "  t1=words[0]\n",
        "  t2=words[1]\n",
        "\n",
        "  if(t1 not in BIGRAM):\n",
        "    list1=[]\n",
        "  else:\n",
        "    list1=BIGRAM[t1]\n",
        "  \n",
        "  if(t2 not in BIGRAM):\n",
        "    list2=[]\n",
        "  else:\n",
        "    list2=BIGRAM[t2]\n",
        "  \n",
        "  sol,comp=solve(list1,list2)\n",
        "  query+=t1+' '+commands[0]+' '+t2\n",
        "\n",
        "      \n",
        "  total+=comp\n",
        "\n",
        "  for i in range(2,len(words)):\n",
        "\n",
        "    t=words[i]\n",
        "\n",
        "    if(t not in BIGRAM):\n",
        "      lst=[]\n",
        "    else:\n",
        "      lst=BIGRAM[t]\n",
        "    \n",
        "    sol,comp=solve(sol,lst,commands[i-1])\n",
        "    \n",
        "    total+=comp\n",
        "    query+=' '+commands[i-1]+' '+t\n",
        "\n",
        "  return query, sol, total  "
      ],
      "metadata": {
        "id": "z6DuYv0E5QVF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AND(list1, list2):\n",
        "  comp=0\n",
        "\n",
        "  if list1==[] or list2==[]:\n",
        "    return [],comp\n",
        "\n",
        "  sol=[]\n",
        "\n",
        "  idx1=0\n",
        "  idx2=0\n",
        "\n",
        "  while idx1<len(list1)  and idx2< len(list2):\n",
        "    comp+=1\n",
        "\n",
        "    if list1[idx1]==list2[idx2]:\n",
        "      sol.append(list1[idx1])\n",
        "      idx1+=1\n",
        "      idx2+=1\n",
        "    \n",
        "    elif list1[idx1]<list2[idx2]:\n",
        "      idx1+=1\n",
        "    \n",
        "    else:\n",
        "      idx2+=1\n",
        "\n",
        "  return sol,comp\n"
      ],
      "metadata": {
        "id": "OQDTPpW3zu1I"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solving preprocessed query\n",
        "\n",
        "def simpler_query(tokens):\n",
        "\n",
        "  words_lst=tokens\n",
        "\n",
        "\n",
        "  words=[words_lst[i]+' '+words_lst[i+1] for i in range(len(words_lst)-1) if (words_lst[i].strip()!='' and words_lst[i+1].strip()!='')]\n",
        "  # print(words)\n",
        "\n",
        "  commands=['AND' for i in range(len(words)-1)]\n",
        "\n",
        "  return compute(words,commands)"
      ],
      "metadata": {
        "id": "BgIP4HqbABOk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to process query\n",
        "\n",
        "def query(no, inp):\n",
        "\n",
        "  tokens=preprocess_query(inp)\n",
        "\n",
        "  if(len(tokens)==0):\n",
        "     print('Number of documents retrieved for query ' +str(no)+' using Bigram Inverted Index : 0')\n",
        "     print('Names of the documents retrieved for query '+ str(no)+ ' using Bigram Inverted Index : None')\n",
        "     return\n",
        "\n",
        "\n",
        "  query, ans, comparisions= simpler_query(tokens)\n",
        "  docNames=''\n",
        "  for id in ans:\n",
        "    docNames+=doc_mapping[id]+' '\n",
        "\n",
        "  \n",
        "  print('Number of documents retrieved for query ' +str(no)+' using Bigram Inverted Index : '+ str(len(ans)))\n",
        "  print('Names of the documents retrieved for query '+ str(no)+ ' using Bigram Inverted Index : '+ docNames)\n",
        "\n"
      ],
      "metadata": {
        "id": "ulEMk2EmBQln"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_query():\n",
        "  t = int(input(\"Enter the number of queries: \"))\n",
        "  idx = 1\n",
        "  while t > 0:\n",
        "    phrase = input(\"Enter the phrase: \")\n",
        "    res_list_using_pi = process(phrase) #Positional\n",
        "    res_list_using_bi = query(idx, phrase) #Bigram\n",
        "\n",
        "    print(f'Number of documents retrieved for query {idx} using positional index: {len(res_list_using_pi)}')\n",
        "    print(f'Names of documents retrieved for query {idx} using positional index: ', end = \" \")\n",
        "    for doc_id in res_list_using_pi:\n",
        "      print(doc_id_mapping[doc_id], end=\", \")\n",
        "    print()\n",
        "    idx+=1\n",
        "    t-=1\n",
        "run_query()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YENV1KP-HBX3",
        "outputId": "3ddc1626-3129-4571-f2a9-f22b560b1731"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 2\n",
            "Enter the phrase: this is in\n",
            "Number of documents retrieved for query 1 using Bigram Inverted Index : 0\n",
            "Names of the documents retrieved for query 1 using Bigram Inverted Index : None\n",
            "Number of documents retrieved for query 1 using positional index: 0\n",
            "Names of documents retrieved for query 1 using positional index:  \n",
            "Enter the phrase: in a the\n",
            "Number of documents retrieved for query 2 using Bigram Inverted Index : 0\n",
            "Names of the documents retrieved for query 2 using Bigram Inverted Index : None\n",
            "Number of documents retrieved for query 2 using positional index: 0\n",
            "Names of documents retrieved for query 2 using positional index:  \n"
          ]
        }
      ]
    }
  ]
}