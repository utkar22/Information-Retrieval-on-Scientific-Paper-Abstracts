{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1sP8jcILzbQGlhbmCWAEOzNbEzu5idPLx",
      "authorship_tag": "ABX9TyPI2UeanYoTw7aJATuRIGdL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkar22/CSE508_Winter2023_A1_48/blob/main/Preprcossing_and_Postional_Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "metadata": {
        "id": "mhii-9ZVZLtb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0518cb2-0493-4864-9d95-c94319e957b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiWFmcmwGINS",
        "outputId": "9b38d30b-15bf-4e93-8747-eabab123bd84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zB6Fq5zFvr1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "class preprocessor:\n",
        "  def __init__(self):\n",
        "    self.value = 1\n",
        "  \n",
        "  def read_file(self, path:str) -> str:\n",
        "    '''reads the contents of a file and converts them into \n",
        "    a string of words separated by space.'''\n",
        "    f = open(path, 'r')\n",
        "    text = ''\n",
        "    for line in f:\n",
        "      for word in line.split(\" \"):\n",
        "        text += '' + word + ' '\n",
        "    f.close()\n",
        "    return text\n",
        "  \n",
        "  def tokenize(self, text:str):\n",
        "    ''' The function takes in a string and \n",
        "    converts it into a list of tokens using nltk'''\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "  def tokens_to_text(self, tokens:List) -> str:\n",
        "    ''' converts the tokens into space-separated string'''\n",
        "    text = ''\n",
        "    for token in tokens:\n",
        "\n",
        "      text += '' + token + ' '\n",
        "    return text\n",
        "\n",
        "  def lowercase(self, tokens:List):\n",
        "    ''' This function takes in the list of tokens\n",
        "     and returns the lowercase version of each token without changing the index positions'''\n",
        "\n",
        "    for i in range(len(tokens)):\n",
        "      tokens[i] = tokens[i].lower()\n",
        "    return tokens\n",
        "\n",
        "  def remove_stopwords(self, tokens:List) -> List:\n",
        "    ''' removes any stop words present as tokens in the list of tokens \n",
        "    present using nltk's corpus of stopwords'''\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    resultant_tokens = []\n",
        "    for token in tokens:\n",
        "      if token not in stop_words:\n",
        "        resultant_tokens.append(token)\n",
        "    return resultant_tokens\n",
        "\n",
        "  def remove_puncts(self, tokens:List) -> List:\n",
        "    ''' removes any punctuations present as tokens \n",
        "    and returns the resutlant list of tokens'''\n",
        "\n",
        "    tokenizer = RegexpTokenizer(r'\\w+') # filters out tokens with any non-alphanumeric characters in them \n",
        "    resultant_tokens = []\n",
        "    text = self.tokens_to_text(tokens)\n",
        "    resultant_tokens = tokenizer.tokenize(text)\n",
        "    return resultant_tokens\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "ByjlLbZISfEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test processor here:"
      ],
      "metadata": {
        "id": "uxgmO2wrcmZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_doc = '/content/drive/MyDrive/IR Assigns/A1/CSE508_Winter2023_Dataset' + f'/cranfield0663'\n",
        "proc = preprocessor()\n",
        "text = proc.read_file(path_to_doc)\n",
        "tokens = proc.tokenize(text)\n",
        "final_list = proc.remove_stopwords(tokens)\n",
        "print(len(final_list))\n",
        "final_list = proc.remove_puncts(final_list)\n",
        "print(final_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yttb1YtgUMfu",
        "outputId": "c0da8633-cd1d-42ad-ca57-ca8289d19e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88\n",
            "['viscous', 'flow', 'along', 'flat', 'plate', 'moving', 'high', 'speeds', 'distortion', 'coordinates', 'shown', 'case', 'supersonic', 'viscous', 'flow', 'past', 'flat', 'plate', 'boundary', 'layer', 'simple', 'wave', 'theories', 'combined', 'give', 'complete', 'representation', 'velocity', 'pressure', 'fields', 'consistent', 'first', 'order', 'solutions', 'considered', 'expression', 'induced', 'pressure', 'plate', 'correct', 'second', 'order', 'obtained', 'high', 'mach', 'numbers', 'important', 'parameter', 'satisfies', 'hypersonic', 'similarity', 'law', 'arbitrary', 'mach', 'reynolds', 'numbers', 'different', 'gases', 'theoretical', 'curve', 'correlates', 'closely', 'experimental', 'data', 'asymptotic', 'shock', 'curve', 'skin', 'friction', 'coefficient', 'also', 'deduced', 'experimental', 'verifications', 'yet', 'made']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DONT RUN THE CELLS BELOW FR FR**"
      ],
      "metadata": {
        "id": "F7aJXm-evsIh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSjME9gXLSvV",
        "outputId": "803dce8a-65cc-447b-cd76-3c0d55bc7945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cranfield0662\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ds = '/content/drive/MyDrive/IR Assigns/A1/CSE508_Winter2023_Dataset'\n",
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/A1/CSE508_Winter2023_Dataset')\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  if doc_name != 'cranfield0662':\n",
        "    continue\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/A1/CSE508_Winter2023_Dataset' + f'/{doc_name}'\n",
        "  f = open(path_to_doc, 'r')\n",
        "  words = []\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      words.append(word)\n",
        "  idx = 0\n",
        "  body = ''\n",
        "  while idx<len(words):\n",
        "    if words[idx] == '<TITLE>':\n",
        "      idx+=1\n",
        "      while idx < len(words) and words[idx]!='</TITLE>':\n",
        "        body += words[idx]+' '\n",
        "        idx+=1\n",
        "    if idx<len(words) and words[idx] == '<TEXT>':\n",
        "      idx+=1\n",
        "      while idx<len(words) and words[idx]!='</TEXT>':\n",
        "        body += words[idx]+' '\n",
        "        idx+=1\n",
        "    idx+=1\n",
        "  \n",
        "  f.close()\n",
        "  open(path_to_doc, \"w\").close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  f.write(body)\n",
        "  f.close()\n",
        "  print(doc_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n"
      ],
      "metadata": {
        "id": "8ZgWuZT1Frgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lowercase Words"
      ],
      "metadata": {
        "id": "YPNC_1zxLHV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = '/content/drive/MyDrive/IR Assigns/toy_ds'\n",
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/toy_ds')\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  f = open(path_to_doc, 'r')\n",
        "  words = []\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      words.append(word.lower())\n",
        "  # print(words)\n",
        "  idx = 0\n",
        "  body = ''\n",
        "  while(idx<len(words)):\n",
        "    body += words[idx] + ' '\n",
        "    if (idx+1)%20 ==0:\n",
        "      body+= '\\n '\n",
        "    idx+=1\n",
        "  f.close()\n",
        "  open(path_to_doc, 'w').close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  f.write(body)\n",
        "  f.close()\n",
        "  # print(doc_name)\n",
        "  "
      ],
      "metadata": {
        "id": "mEp-wpiLsEBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilSnA7ASsgnI",
        "outputId": "7eedbd4e-5d8b-417f-a1d2-6cf18e52f208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform tokenisation and save the tokens in the files"
      ],
      "metadata": {
        "id": "JLRyDE7RTsI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/toy_ds')\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  f = open(path_to_doc, 'r')\n",
        "  text = ''\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      text += ' ' + word +''\n",
        "  tokens = word_tokenize(text)\n",
        "  idx = 0\n",
        "  to_write = ''# for demo. no point in making tokens and then again saving in the file \n",
        "  while(idx<len(tokens)):\n",
        "    to_write += tokens[idx] + ' '\n",
        "    idx+=1\n",
        "  f.close()\n",
        "  open(path_to_doc, 'w').close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  \n",
        "  f.write(to_write)\n",
        "  f.close()\n",
        "  # print(doc_name)"
      ],
      "metadata": {
        "id": "WTZLAKXDNBbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGqM9V2rSl4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing stop words and saving the tokens in the file itself"
      ],
      "metadata": {
        "id": "Dg6FRmWvTmni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/toy_ds')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  f = open(path_to_doc, 'r')\n",
        "  text = ''\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      text += ' ' + word +''\n",
        "  tokens = word_tokenize(text)\n",
        "  idx = 0\n",
        "  to_write = ''\n",
        "  while(idx<len(tokens)):\n",
        "    if tokens[idx] not in stop_words:\n",
        "      to_write += tokens[idx]+ ' '\n",
        "    idx+=1\n",
        "  f.close()\n",
        "  open(path_to_doc, 'w').close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  \n",
        "  f.write(to_write)\n",
        "  f.close()\n",
        "  # print(doc_name)\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "_mSxjfNQSbHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Puncs And Saving In The Files"
      ],
      "metadata": {
        "id": "W9LsmjvBUKeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "g49glEHqUNHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+') # filters out tokens with any non-alphanumeric characters in them \n",
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/toy_ds')\n",
        "\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  f = open(path_to_doc, 'r')\n",
        "  text = ''\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      text += ' ' + word +''\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  idx = 0\n",
        "  to_write = ''\n",
        "  while(idx<len(tokens)):\n",
        "    to_write += tokens[idx]+ ' '\n",
        "    idx+=1\n",
        "  f.close()\n",
        "  open(path_to_doc, 'w').close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  \n",
        "  f.write(to_write)\n",
        "  f.close()\n",
        "  # print(doc_name)\n",
        "  "
      ],
      "metadata": {
        "id": "MOSFAdUPeTWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing blank spaces"
      ],
      "metadata": {
        "id": "cc8LWPqhfnXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_name_list = []\n",
        "\n",
        "for doc in os.listdir('/content/drive/MyDrive/IR Assigns/toy_ds'):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  doc_name_list.append(doc_name)\n",
        "  f = open(path_to_doc, 'r')\n",
        "  text = ''\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      text += ' ' + word +''\n",
        "  tokens = word_tokenize(text)\n",
        "  print(tokens)\n",
        "  break\n",
        "  print(doc_name)\n",
        "  "
      ],
      "metadata": {
        "id": "_iZS2iihffT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acebc32d-788a-41a3-ba8e-0193617c9a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['doc', 'docno', '25', 'docno', 'title', 'inviscid', 'hypersonic', 'flow', 'blunt', 'nosed', 'slender', 'bodies', 'title', 'author', 'lees', 'l', 'kubota', 'author', 'biblio', 'j', 'ae', 'scs', '24', '1957', '195', 'biblio', 'text', 'hypersonic', 'speeds', 'drag', 'area', 'blunt', 'nose', 'much', 'larger', 'drag', 'area', 'slender', 'afterbody', 'energy', 'contained', 'flow', 'field', 'plane', 'right', 'angles', 'flight', 'direction', 'nearly', 'constant', 'downstream', 'distance', 'many', 'times', 'greater', 'characteristic', 'nose', 'dimension', 'transverse', 'flow', 'field', 'exhibits', 'certain', 'similarity', 'properties', 'directly', 'analogous', 'flow', 'similarity', 'behind', 'intense', 'blast', 'wave', 'found', 'g', 'taylor', 'c', 'lin', 'sakurai', 'comparison', 'experiments', 'hammitt', 'vas', 'bogdonoff', 'flat', 'plate', 'blunt', 'leading', 'edge', 'helium', 'shows', 'shock', 'wave', 'shape', 'predicted', 'accurately', 'similarity', 'analysis', 'predicted', 'surface', 'pressure', 'distribution', 'somewhat', 'less', 'satisfactory', 'experimental', 'results', 'hemisphere', 'cylinder', 'obtained', 'galcit', 'air', 'tunnel', 'indicate', 'shock', 'wave', 'shape', 'also', 'surface', 'pressures', 'body', 'given', 'closely', 'similarity', 'theory', 'except', 'near', 'hemisphere', 'cylinder', 'junction', 'energy', 'considerations', 'combined', 'detailed', 'study', 'equations', 'motion', 'show', 'flow', 'similarity', 'also', 'possible', 'class', 'bodies', 'form', 'provided', 'two', 'dimensional', 'body', 'body', 'revolution', 'shock', 'shape', 'similar', 'body', 'shape', 'entire', 'flow', 'field', 'distance', 'nose', 'must', 'depend', 'extent', 'details', 'nose', 'geometry', 'utilizing', 'energy', 'drag', 'considerations', 'one', 'finds', 'hypersonic', 'speeds', 'inviscid', 'surface', 'pressures', 'generated', 'blunt', 'leading', 'edge', 'larger', 'pressures', 'induced', 'boundary', 'layer', 'growth', 'insulated', 'flat', 'surface', 'insulated', 'blunt', 'nosed', 'slender', 'body', 'revolution', 'corresponding', 'distance', 'given', 'free', 'stream', 'reynolds', 'number', 'based', 'leading', 'edge', 'thickness', 'nose', 'diameter', 'free', 'flight', 'constants', 'replaced', '1', '700', '20', 'respectively', 'viscous', 'interaction', 'effects', 'important', 'forward', 'portion', 'bluntnosed', 'slender', 'body', 'relatively', 'low', 'values', 'however', 'far', 'downstream', 'nose', 'inviscid', 'pressure', 'small', 'viscous', 'interaction', 'phenomena', 'taken', 'account', 'text', 'doc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word:\n",
        "\n",
        "  list of docs:\n",
        "  \n",
        "    lsit of positions\n"
      ],
      "metadata": {
        "id": "E5tU5SRfgmgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(doc_name_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RolQ0ux_scTU",
        "outputId": "eaf6e475-7d91-4c69-eebe-0402d717deb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class word:\n",
        "  def __init__(self):\n",
        "    self.value = None\n",
        "    self.freq = 1\n",
        "    self.docs = {}\n",
        "class doc:\n",
        "  def __init__(self, did):\n",
        "    self.docId = did\n",
        "    self.positions = []\n",
        "dictionary = {}"
      ],
      "metadata": {
        "id": "DBk7OrVmhn2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_mapping = {}\n",
        "for i in range(len(doc_name_list)):\n",
        "  doc_mapping[i+1] = doc_name_list[i]\n",
        "print(doc_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jgDHpN3sPnf",
        "outputId": "f02caedc-9e90-40f7-ccbb-5376cb9fce86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'cranfield0025', 2: 'cranfield0013', 3: 'cranfield0015', 4: 'cranfield0016', 5: 'cranfield0017', 6: 'cranfield0022', 7: 'cranfield0014', 8: 'cranfield0012', 9: 'cranfield0021', 10: 'cranfield0024', 11: 'cranfield0023', 12: 'cranfield0019', 13: 'cranfield0006', 14: 'cranfield0011', 15: 'cranfield0020', 16: 'cranfield0018', 17: 'cranfield0002', 18: 'cranfield0007', 19: 'cranfield0008', 20: 'cranfield0004', 21: 'cranfield0009', 22: 'cranfield0005', 23: 'cranfield0010', 24: 'cranfield0003', 25: 'cranfield0001'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proc = preprocessor()\n",
        "for doc_id in doc_mapping.keys():\n",
        "  doc_name = doc_mapping[doc_id]\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  text = proc.read_file(path_to_doc)\n",
        "  tokens = proc.tokenize(text)\n",
        "  for idx  in range(len(tokens)):\n",
        "    token = tokens[idx]\n",
        "    if token in dictionary:\n",
        "      element = dictionary[token]\n",
        "      if doc_id in element.docs:\n",
        "        element.docs[doc_id].positions.append(idx)\n",
        "      else:\n",
        "        element.docs[doc_id] = doc(doc_id)\n",
        "        document = dictionary[token].docs[doc_id]\n",
        "        document.positions.append(idx)\n",
        "        element.freq= len(dictionary[token].docs)\n",
        "    else:\n",
        "      dictionary[token] = word()\n",
        "      element = dictionary[token]\n",
        "      element.value = token\n",
        "      element.docs[doc_id] = doc(doc_id)\n",
        "      document = element.docs[doc_id]\n",
        "      document.positions.append(idx)\n",
        "print(dictionary['doc'].docs)\n",
        "print(dictionary['doc'].freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jGv5d2njfnU",
        "outputId": "dc8a44e7-1cd7-469f-bfcc-4f8e5860ae16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: <__main__.doc object at 0x7f771a79adc0>, 2: <__main__.doc object at 0x7f771a818ac0>, 3: <__main__.doc object at 0x7f771a7de730>, 4: <__main__.doc object at 0x7f771a843c40>, 5: <__main__.doc object at 0x7f771a826dc0>, 6: <__main__.doc object at 0x7f771a87ae80>, 7: <__main__.doc object at 0x7f771a85a6a0>, 8: <__main__.doc object at 0x7f771aa1c7f0>, 9: <__main__.doc object at 0x7f771a8cbf10>, 10: <__main__.doc object at 0x7f771a8d35b0>, 11: <__main__.doc object at 0x7f771a8ca1f0>, 12: <__main__.doc object at 0x7f771a8c4130>, 13: <__main__.doc object at 0x7f771a84e490>, 14: <__main__.doc object at 0x7f771a8460d0>, 15: <__main__.doc object at 0x7f771a841250>, 16: <__main__.doc object at 0x7f771a8383d0>, 17: <__main__.doc object at 0x7f771a8361f0>, 18: <__main__.doc object at 0x7f771a81f490>, 19: <__main__.doc object at 0x7f771a807eb0>, 20: <__main__.doc object at 0x7f771a7ef3d0>, 21: <__main__.doc object at 0x7f771a7f7c70>, 22: <__main__.doc object at 0x7f771a7e7190>, 23: <__main__.doc object at 0x7f771a7d9310>, 24: <__main__.doc object at 0x7f770cd248b0>, 25: <__main__.doc object at 0x7f770cd26bb0>}\n",
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def intersect(word1:word, word2:word)->List:\n",
        "  ''' gives list of documents that contain both word1 and word2'''\n",
        "  dict1, dict2 = dictionary[word1].docs, dictionary[word2].docs\n",
        "  common_docs = []\n",
        "  common_docs = list(dict1.keys() & dict2.keys())\n",
        "  return common_docs\n"
      ],
      "metadata": {
        "id": "C4tJIC6yg10E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proc = preprocessor()\n",
        "def preprocess_query(phrase:str)->List:\n",
        "  phrase_tokens = proc.tokenize(phrase)\n",
        "  phrase_tokens = proc.lowercase(phrase_tokens)\n",
        "  phrase_tokens = proc.remove_stopwords(phrase_tokens)\n",
        "  phrase_tokens = proc.remove_puncts(phrase_tokens)\n",
        "  return phrase_tokens\n",
        "\n",
        "def intersect_docs(phrase:str, dictionary:dict, cands:set):\n",
        "  phrase_tokens = preprocess_query(phrase)\n",
        "  for token in phrase_tokens:\n",
        "    if token in dictionary.keys():\n",
        "      cands = cands & set(dictionary[token].docs)\n",
        "    else:\n",
        "      return {}\n",
        "  return cands\n",
        "\n",
        "phrase = 'study'\n",
        "candidates = set(list(range(1, len(dictionary)+1)))"
      ],
      "metadata": {
        "id": "45kw9k3Cqx24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(tokens:List, candidate_docs:List)->List:\n",
        "  res_list = []\n",
        "  for doc_id in candidate_docs:\n",
        "    inter_list = dictionary[tokens[0]].docs[doc_id].positions # go to word -> doc_id -> postion list\n",
        "    for idx in range(1, len(tokens)):\n",
        "      pos_list = dictionary[tokens[idx]].docs[doc_id].positions\n",
        "      inter_list_copy = set([x+1 for x in inter_list])\n",
        "      inter_list =  list(inter_list_copy & set(pos_list))\n",
        "    if len(inter_list) != 0:\n",
        "      res_list.append(doc_id)\n",
        "  return res_list\n",
        "phrase = 'body shape'\n",
        "phrase_tokens = preprocess_query(phrase)  \n",
        "candidate_docs = intersect_docs(phrase, dictionary, candidates)  \n",
        "print(f'the candidate docs are: {candidate_docs}')\n",
        "res_list = process(phrase_tokens, candidate_docs)\n",
        "print(res_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xlee-7LjNFgi",
        "outputId": "a4430667-2f25-4cf3-8379-b142cc78118f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the candidate docs are: {1}\n",
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSkEUHRq59MD",
        "outputId": "9a236a0b-cbdf-4274-fbf3-7849881d807f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 24, 300]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iZy8yXl4e-U",
        "outputId": "b17fbfe4-b1f1-4ac3-b337-2f99f8d8aa92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.10\n"
          ]
        }
      ]
    }
  ]
}