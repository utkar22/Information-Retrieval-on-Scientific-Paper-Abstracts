{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1sP8jcILzbQGlhbmCWAEOzNbEzu5idPLx",
      "authorship_tag": "ABX9TyN+InzlbUT22fOfiRsLHklm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkar22/CSE508_Winter2023_A1_48/blob/main/IR_A1_Abhey_Kalia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "mhii-9ZVZLtb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiWFmcmwGINS",
        "outputId": "f1a8fd2a-7c08-4506-82bc-817661d30666"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DONT RUN THE CELLS BELOW FR FR**"
      ],
      "metadata": {
        "id": "F7aJXm-evsIh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zB6Fq5zFvr1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSjME9gXLSvV",
        "outputId": "803dce8a-65cc-447b-cd76-3c0d55bc7945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cranfield0662\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ds = '/content/drive/MyDrive/IR Assigns/A1/CSE508_Winter2023_Dataset'\n",
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/A1/CSE508_Winter2023_Dataset')\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  if doc_name != 'cranfield0662':\n",
        "    continue\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/A1/CSE508_Winter2023_Dataset' + f'/{doc_name}'\n",
        "  f = open(path_to_doc, 'r')\n",
        "  words = []\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      words.append(word)\n",
        "  idx = 0\n",
        "  body = ''\n",
        "  while idx<len(words):\n",
        "    if words[idx] == '<TITLE>':\n",
        "      idx+=1\n",
        "      while idx < len(words) and words[idx]!='</TITLE>':\n",
        "        body += words[idx]+' '\n",
        "        idx+=1\n",
        "    if idx<len(words) and words[idx] == '<TEXT>':\n",
        "      idx+=1\n",
        "      while idx<len(words) and words[idx]!='</TEXT>':\n",
        "        body += words[idx]+' '\n",
        "        idx+=1\n",
        "    idx+=1\n",
        "  \n",
        "  f.close()\n",
        "  open(path_to_doc, \"w\").close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  f.write(body)\n",
        "  f.close()\n",
        "  print(doc_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n"
      ],
      "metadata": {
        "id": "8ZgWuZT1Frgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lowercase Words"
      ],
      "metadata": {
        "id": "YPNC_1zxLHV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = '/content/drive/MyDrive/IR Assigns/toy_ds'\n",
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/toy_ds')\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  f = open(path_to_doc, 'r')\n",
        "  words = []\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      words.append(word.lower())\n",
        "  # print(words)\n",
        "  idx = 0\n",
        "  body = ''\n",
        "  while(idx<len(words)):\n",
        "    body += words[idx] + ' '\n",
        "    if (idx+1)%20 ==0:\n",
        "      body+= '\\n '\n",
        "    idx+=1\n",
        "  f.close()\n",
        "  open(path_to_doc, 'w').close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  f.write(body)\n",
        "  f.close()\n",
        "  print(doc_name)\n",
        "  "
      ],
      "metadata": {
        "id": "mEp-wpiLsEBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce7b99d-be4e-42db-999b-0f4098ba4d3e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cranfield0025\n",
            "cranfield0023\n",
            "cranfield0022\n",
            "cranfield0024\n",
            "cranfield0021\n",
            "cranfield0015\n",
            "cranfield0017\n",
            "cranfield0012\n",
            "cranfield0013\n",
            "cranfield0016\n",
            "cranfield0014\n",
            "cranfield0019\n",
            "cranfield0020\n",
            "cranfield0018\n",
            "cranfield0011\n",
            "cranfield0006\n",
            "cranfield0009\n",
            "cranfield0004\n",
            "cranfield0007\n",
            "cranfield0008\n",
            "cranfield0002\n",
            "cranfield0005\n",
            "cranfield0003\n",
            "cranfield0010\n",
            "cranfield0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilSnA7ASsgnI",
        "outputId": "7dc2b9b8-e6de-4203-89aa-914045b4b589"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform tokenisation and save the tokens in the files"
      ],
      "metadata": {
        "id": "JLRyDE7RTsI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/toy_ds')\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  f = open(path_to_doc, 'r')\n",
        "  text = ''\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      text += ' ' + word +''\n",
        "  tokens = word_tokenize(text)\n",
        "  idx = 0\n",
        "  to_write = ''# for demo. no point in making tokens and then again saving in the file \n",
        "  while(idx<len(tokens)):\n",
        "    to_write += tokens[idx] + ' '\n",
        "    idx+=1\n",
        "  f.close()\n",
        "  open(path_to_doc, 'w').close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  \n",
        "  f.write(to_write)\n",
        "  f.close()\n",
        "  print(doc_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTZLAKXDNBbx",
        "outputId": "2b13e84f-8e9d-4f7b-f4e4-941a63c2f005"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cranfield0025\n",
            "cranfield0023\n",
            "cranfield0022\n",
            "cranfield0024\n",
            "cranfield0021\n",
            "cranfield0015\n",
            "cranfield0017\n",
            "cranfield0012\n",
            "cranfield0013\n",
            "cranfield0016\n",
            "cranfield0014\n",
            "cranfield0019\n",
            "cranfield0020\n",
            "cranfield0018\n",
            "cranfield0011\n",
            "cranfield0006\n",
            "cranfield0009\n",
            "cranfield0004\n",
            "cranfield0007\n",
            "cranfield0008\n",
            "cranfield0002\n",
            "cranfield0005\n",
            "cranfield0003\n",
            "cranfield0010\n",
            "cranfield0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "cGqM9V2rSl4z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing stop words and saving the tokens in the file itself"
      ],
      "metadata": {
        "id": "Dg6FRmWvTmni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/toy_ds')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  f = open(path_to_doc, 'r')\n",
        "  text = ''\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      text += ' ' + word +''\n",
        "  tokens = word_tokenize(text)\n",
        "  idx = 0\n",
        "  to_write = ''\n",
        "  while(idx<len(tokens)):\n",
        "    if tokens[idx] not in stop_words:\n",
        "      to_write += tokens[idx]+ ' '\n",
        "    idx+=1\n",
        "  f.close()\n",
        "  open(path_to_doc, 'w').close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  \n",
        "  f.write(to_write)\n",
        "  f.close()\n",
        "  print(doc_name)\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mSxjfNQSbHp",
        "outputId": "e6b7c73e-a02f-4d28-d1d7-045e07af966c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cranfield0025\n",
            "cranfield0023\n",
            "cranfield0022\n",
            "cranfield0024\n",
            "cranfield0021\n",
            "cranfield0015\n",
            "cranfield0017\n",
            "cranfield0012\n",
            "cranfield0013\n",
            "cranfield0016\n",
            "cranfield0014\n",
            "cranfield0019\n",
            "cranfield0020\n",
            "cranfield0018\n",
            "cranfield0011\n",
            "cranfield0006\n",
            "cranfield0009\n",
            "cranfield0004\n",
            "cranfield0007\n",
            "cranfield0008\n",
            "cranfield0002\n",
            "cranfield0005\n",
            "cranfield0003\n",
            "cranfield0010\n",
            "cranfield0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Puncs And Saving In The Files"
      ],
      "metadata": {
        "id": "W9LsmjvBUKeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n"
      ],
      "metadata": {
        "id": "g49glEHqUNHC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+') # filters out tokens with any non-alphanumeric characters in them \n",
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/toy_ds')\n",
        "\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  f = open(path_to_doc, 'r')\n",
        "  text = ''\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      text += ' ' + word +''\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  idx = 0\n",
        "  to_write = ''\n",
        "  while(idx<len(tokens)):\n",
        "    to_write += tokens[idx]+ ' '\n",
        "    idx+=1\n",
        "  f.close()\n",
        "  open(path_to_doc, 'w').close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  \n",
        "  f.write(to_write)\n",
        "  f.close()\n",
        "  print(doc_name)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOSFAdUPeTWd",
        "outputId": "8102ffed-932d-450a-c282-2748c9d6ce98"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cranfield0025\n",
            "cranfield0023\n",
            "cranfield0022\n",
            "cranfield0024\n",
            "cranfield0021\n",
            "cranfield0015\n",
            "cranfield0017\n",
            "cranfield0012\n",
            "cranfield0013\n",
            "cranfield0016\n",
            "cranfield0014\n",
            "cranfield0019\n",
            "cranfield0020\n",
            "cranfield0018\n",
            "cranfield0011\n",
            "cranfield0006\n",
            "cranfield0009\n",
            "cranfield0004\n",
            "cranfield0007\n",
            "cranfield0008\n",
            "cranfield0002\n",
            "cranfield0005\n",
            "cranfield0003\n",
            "cranfield0010\n",
            "cranfield0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing blank spaces"
      ],
      "metadata": {
        "id": "cc8LWPqhfnXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  f = open(path_to_doc, 'r')\n",
        "  text = ''\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      text += ' ' + word +''\n",
        "  tokens = word_tokenize(text)\n",
        "  print(tokens)\n",
        "  print(doc_name)\n",
        "  break\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iZS2iihffT2",
        "outputId": "41b7dade-8320-41b9-9360-14f10893a10c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['doc', 'docno', '25', 'docno', 'title', 'inviscid', 'hypersonic', 'flow', 'blunt', 'nosed', 'slender', 'bodies', 'title', 'author', 'lees', 'l', 'kubota', 'author', 'biblio', 'j', 'ae', 'scs', '24', '1957', '195', 'biblio', 'text', 'hypersonic', 'speeds', 'drag', 'area', 'blunt', 'nose', 'much', 'larger', 'drag', 'area', 'slender', 'afterbody', 'energy', 'contained', 'flow', 'field', 'plane', 'right', 'angles', 'flight', 'direction', 'nearly', 'constant', 'downstream', 'distance', 'many', 'times', 'greater', 'characteristic', 'nose', 'dimension', 'transverse', 'flow', 'field', 'exhibits', 'certain', 'similarity', 'properties', 'directly', 'analogous', 'flow', 'similarity', 'behind', 'intense', 'blast', 'wave', 'found', 'g', 'i', 'taylor', 's', 'c', 'lin', 'a', 'sakurai', 'comparison', 'experiments', 'hammitt', 'vas', 'bogdonoff', 'flat', 'plate', 'blunt', 'leading', 'edge', 'helium', 'shows', 'shock', 'wave', 'shape', 'predicted', 'accurately', 'similarity', 'analysis', 'predicted', 'surface', 'pressure', 'distribution', 'somewhat', 'less', 'satisfactory', 'experimental', 'results', 'hemisphere', 'cylinder', 'obtained', 'galcit', 'air', 'tunnel', 'indicate', 'shock', 'wave', 'shape', 'also', 'surface', 'pressures', 'body', 'given', 'closely', 'similarity', 'theory', 'except', 'near', 'hemisphere', 'cylinder', 'junction', 'energy', 'considerations', 'combined', 'detailed', 'study', 'equations', 'motion', 'show', 'flow', 'similarity', 'also', 'possible', 'class', 'bodies', 'form', 'provided', 'two', 'dimensional', 'body', 'body', 'revolution', 'shock', 'shape', 'similar', 'body', 'shape', 'entire', 'flow', 'field', 'distance', 'nose', 'must', 'depend', 'extent', 'details', 'nose', 'geometry', 'utilizing', 'energy', 'drag', 'considerations', 'one', 'finds', 'hypersonic', 'speeds', 'inviscid', 'surface', 'pressures', 'generated', 'blunt', 'leading', 'edge', 'larger', 'pressures', 'induced', 'boundary', 'layer', 'growth', 'insulated', 'flat', 'surface', 'insulated', 'blunt', 'nosed', 'slender', 'body', 'revolution', 'corresponding', 'distance', 'given', 'free', 'stream', 'reynolds', 'number', 'based', 'leading', 'edge', 'thickness', 'nose', 'diameter', 'free', 'flight', 'constants', 'replaced', '1', '700', '20', 'respectively', 'viscous', 'interaction', 'effects', 'important', 'forward', 'portion', 'bluntnosed', 'slender', 'body', 'relatively', 'low', 'values', 'however', 'far', 'downstream', 'nose', 'inviscid', 'over', 'pressure', 'small', 'viscous', 'interaction', 'phenomena', 'taken', 'account', 'text', 'doc']\n",
            "cranfield0025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word:\n",
        "\n",
        "  list of docs:\n",
        "  \n",
        "    lsit of positions\n"
      ],
      "metadata": {
        "id": "E5tU5SRfgmgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class word:\n",
        "  def __init__(self, toi):\n",
        "    self.value = toi\n",
        "    self.freq = 1\n",
        "    self.docs = {}\n",
        "class doc:\n",
        "  def __init__(self, did):\n",
        "    self.docId = did\n",
        "    self.positions = []\n",
        "print(doc(1))\n",
        "dictionary = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBk7OrVmhn2x",
        "outputId": "afe16f07-c20f-41d0-ab51-7aee561e0071"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.doc object at 0x7fcf3655e2b0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_id = 1\n",
        "for idx  in range(len(tokens)):\n",
        "  token = tokens[idx]\n",
        "  if token in dictionary:\n",
        "    element = dictionary[token]\n",
        "    if doc_id in element.docs:\n",
        "      element.docs[doc_id].positions.append(idx)\n",
        "    else:\n",
        "      element.docs[doc_id] = doc(doc_id)\n",
        "      document = dictionary[token].docs[doc_id]\n",
        "      document.positions.append(idx)\n",
        "      element.freq= len(dictionary[token].docs)\n",
        "  else:\n",
        "    dictionary[token] = word(token)\n",
        "    element = dictionary[token]\n",
        "    element.value = token\n",
        "    element.docs[doc_id] = doc(doc_id)\n",
        "    document = element.docs[doc_id]\n",
        "    document.positions.append(idx)\n",
        "print(dictionary['doc'].docs[doc_id].positions)\n",
        "print(dictionary['doc'].freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jGv5d2njfnU",
        "outputId": "dc47962f-f4d4-4230-fb78-97f1278bd5b1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 247, 250]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dictionary['viscous'].docs[doc_id].positions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlKpfiWhrbq5",
        "outputId": "8ee5c866-5bde-45e1-ad30-1ec23744c916"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[221, 241]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5usidBcniGO",
        "outputId": "56c4b141-d018-41a3-dac5-e8a592e7a61a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['doc', 'docno', '25', 'docno', 'title', 'inviscid', 'hypersonic', 'flow', 'blunt', 'nosed', 'slender', 'bodies', 'title', 'author', 'lees', 'l', 'kubota', 'author', 'biblio', 'j', 'ae', 'scs', '24', '1957', '195', 'biblio', 'text', 'hypersonic', 'speeds', 'drag', 'area', 'blunt', 'nose', 'much', 'larger', 'drag', 'area', 'slender', 'afterbody', 'energy', 'contained', 'flow', 'field', 'plane', 'right', 'angles', 'flight', 'direction', 'nearly', 'constant', 'downstream', 'distance', 'many', 'times', 'greater', 'characteristic', 'nose', 'dimension', 'transverse', 'flow', 'field', 'exhibits', 'certain', 'similarity', 'properties', 'directly', 'analogous', 'flow', 'similarity', 'behind', 'intense', 'blast', 'wave', 'found', 'g', 'i', 'taylor', 's', 'c', 'lin', 'a', 'sakurai', 'comparison', 'experiments', 'hammitt', 'vas', 'bogdonoff', 'flat', 'plate', 'blunt', 'leading', 'edge', 'helium', 'shows', 'shock', 'wave', 'shape', 'predicted', 'accurately', 'similarity', 'analysis', 'predicted', 'surface', 'pressure', 'distribution', 'somewhat', 'less', 'satisfactory', 'experimental', 'results', 'hemisphere', 'cylinder', 'obtained', 'galcit', 'air', 'tunnel', 'indicate', 'shock', 'wave', 'shape', 'also', 'surface', 'pressures', 'body', 'given', 'closely', 'similarity', 'theory', 'except', 'near', 'hemisphere', 'cylinder', 'junction', 'energy', 'considerations', 'combined', 'detailed', 'study', 'equations', 'motion', 'show', 'flow', 'similarity', 'also', 'possible', 'class', 'bodies', 'form', 'provided', 'two', 'dimensional', 'body', 'body', 'revolution', 'shock', 'shape', 'similar', 'body', 'shape', 'entire', 'flow', 'field', 'distance', 'nose', 'must', 'depend', 'extent', 'details', 'nose', 'geometry', 'utilizing', 'energy', 'drag', 'considerations', 'one', 'finds', 'hypersonic', 'speeds', 'inviscid', 'surface', 'pressures', 'generated', 'blunt', 'leading', 'edge', 'larger', 'pressures', 'induced', 'boundary', 'layer', 'growth', 'insulated', 'flat', 'surface', 'insulated', 'blunt', 'nosed', 'slender', 'body', 'revolution', 'corresponding', 'distance', 'given', 'free', 'stream', 'reynolds', 'number', 'based', 'leading', 'edge', 'thickness', 'nose', 'diameter', 'free', 'flight', 'constants', 'replaced', '1', '700', '20', 'respectively', 'viscous', 'interaction', 'effects', 'important', 'forward', 'portion', 'bluntnosed', 'slender', 'body', 'relatively', 'low', 'values', 'however', 'far', 'downstream', 'nose', 'inviscid', 'over', 'pressure', 'small', 'viscous', 'interaction', 'phenomena', 'taken', 'account', 'text', 'doc', 'hello', 'docno', 'doc', 'flow']\n"
          ]
        }
      ]
    }
  ]
}