{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1sP8jcILzbQGlhbmCWAEOzNbEzu5idPLx",
      "authorship_tag": "ABX9TyNgKCoeFzr503WstJQN/IRu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkar22/CSE508_Winter2023_A1_48/blob/main/IR_A1_Abhey_Kalia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "mhii-9ZVZLtb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiWFmcmwGINS",
        "outputId": "1c9f5a58-6d82-4f48-e000-2f7bd7580ed9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DONT RUN THE CELLS BELOW FR FR**"
      ],
      "metadata": {
        "id": "F7aJXm-evsIh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zB6Fq5zFvr1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSjME9gXLSvV",
        "outputId": "803dce8a-65cc-447b-cd76-3c0d55bc7945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cranfield0662\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ds = '/content/drive/MyDrive/IR Assigns/A1/CSE508_Winter2023_Dataset'\n",
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/A1/CSE508_Winter2023_Dataset')\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  if doc_name != 'cranfield0662':\n",
        "    continue\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/A1/CSE508_Winter2023_Dataset' + f'/{doc_name}'\n",
        "  f = open(path_to_doc, 'r')\n",
        "  words = []\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      words.append(word)\n",
        "  idx = 0\n",
        "  body = ''\n",
        "  while idx<len(words):\n",
        "    if words[idx] == '<TITLE>':\n",
        "      idx+=1\n",
        "      while idx < len(words) and words[idx]!='</TITLE>':\n",
        "        body += words[idx]+' '\n",
        "        idx+=1\n",
        "    if idx<len(words) and words[idx] == '<TEXT>':\n",
        "      idx+=1\n",
        "      while idx<len(words) and words[idx]!='</TEXT>':\n",
        "        body += words[idx]+' '\n",
        "        idx+=1\n",
        "    idx+=1\n",
        "  \n",
        "  f.close()\n",
        "  open(path_to_doc, \"w\").close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  f.write(body)\n",
        "  f.close()\n",
        "  print(doc_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n"
      ],
      "metadata": {
        "id": "8ZgWuZT1Frgl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lowercase Words"
      ],
      "metadata": {
        "id": "YPNC_1zxLHV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = '/content/drive/MyDrive/IR Assigns/toy_ds'\n",
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/toy_ds')\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  f = open(path_to_doc, 'r')\n",
        "  words = []\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      words.append(word.lower())\n",
        "  # print(words)\n",
        "  idx = 0\n",
        "  body = ''\n",
        "  while(idx<len(words)):\n",
        "    body += words[idx] + ' '\n",
        "    if (idx+1)%20 ==0:\n",
        "      body+= '\\n '\n",
        "    idx+=1\n",
        "  f.close()\n",
        "  open(path_to_doc, 'w').close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  f.write(body)\n",
        "  f.close()\n",
        "  # print(doc_name)\n",
        "  "
      ],
      "metadata": {
        "id": "mEp-wpiLsEBe"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilSnA7ASsgnI",
        "outputId": "9510b82f-f22c-41a8-9343-2cdfc95f9248"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform tokenisation and save the tokens in the files"
      ],
      "metadata": {
        "id": "JLRyDE7RTsI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/toy_ds')\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  f = open(path_to_doc, 'r')\n",
        "  text = ''\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      text += ' ' + word +''\n",
        "  tokens = word_tokenize(text)\n",
        "  idx = 0\n",
        "  to_write = ''# for demo. no point in making tokens and then again saving in the file \n",
        "  while(idx<len(tokens)):\n",
        "    to_write += tokens[idx] + ' '\n",
        "    idx+=1\n",
        "  f.close()\n",
        "  open(path_to_doc, 'w').close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  \n",
        "  f.write(to_write)\n",
        "  f.close()\n",
        "  # print(doc_name)"
      ],
      "metadata": {
        "id": "WTZLAKXDNBbx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "cGqM9V2rSl4z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing stop words and saving the tokens in the file itself"
      ],
      "metadata": {
        "id": "Dg6FRmWvTmni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/toy_ds')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  f = open(path_to_doc, 'r')\n",
        "  text = ''\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      text += ' ' + word +''\n",
        "  tokens = word_tokenize(text)\n",
        "  idx = 0\n",
        "  to_write = ''\n",
        "  while(idx<len(tokens)):\n",
        "    if tokens[idx] not in stop_words:\n",
        "      to_write += tokens[idx]+ ' '\n",
        "    idx+=1\n",
        "  f.close()\n",
        "  open(path_to_doc, 'w').close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  \n",
        "  f.write(to_write)\n",
        "  f.close()\n",
        "  # print(doc_name)\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "_mSxjfNQSbHp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Puncs And Saving In The Files"
      ],
      "metadata": {
        "id": "W9LsmjvBUKeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n"
      ],
      "metadata": {
        "id": "g49glEHqUNHC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+') # filters out tokens with any non-alphanumeric characters in them \n",
        "dataset = os.fsencode('/content/drive/MyDrive/IR Assigns/toy_ds')\n",
        "\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  f = open(path_to_doc, 'r')\n",
        "  text = ''\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      text += ' ' + word +''\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  idx = 0\n",
        "  to_write = ''\n",
        "  while(idx<len(tokens)):\n",
        "    to_write += tokens[idx]+ ' '\n",
        "    idx+=1\n",
        "  f.close()\n",
        "  open(path_to_doc, 'w').close()\n",
        "  f = open(path_to_doc, 'w')\n",
        "  \n",
        "  f.write(to_write)\n",
        "  f.close()\n",
        "  # print(doc_name)\n",
        "  "
      ],
      "metadata": {
        "id": "MOSFAdUPeTWd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing blank spaces"
      ],
      "metadata": {
        "id": "cc8LWPqhfnXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_name_list = []\n",
        "\n",
        "for doc in os.listdir(dataset):\n",
        "  doc_name = os.fsdecode(doc)\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  if doc_name.find('.ipynb') !=-1:\n",
        "    continue\n",
        "  doc_name_list.append(doc_name)\n",
        "  f = open(path_to_doc, 'r')\n",
        "  text = ''\n",
        "  for line in f:\n",
        "    for word in line.split():\n",
        "      text += ' ' + word +''\n",
        "  tokens = word_tokenize(text)\n",
        "  # print(tokens)\n",
        "  # print(doc_name)\n",
        "  "
      ],
      "metadata": {
        "id": "_iZS2iihffT2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word:\n",
        "\n",
        "  list of docs:\n",
        "  \n",
        "    lsit of positions\n"
      ],
      "metadata": {
        "id": "E5tU5SRfgmgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(doc_name_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RolQ0ux_scTU",
        "outputId": "59d1faf2-1a00-48c2-c243-3567e366b05e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class word:\n",
        "  def __init__(self):\n",
        "    self.value = None\n",
        "    self.freq = 1\n",
        "    self.docs = {}\n",
        "class doc:\n",
        "  def __init__(self, did):\n",
        "    self.docId = did\n",
        "    self.positions = []\n",
        "dictionary = {}"
      ],
      "metadata": {
        "id": "DBk7OrVmhn2x"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_mapping = {}\n",
        "for i in range(len(doc_name_list)):\n",
        "  doc_mapping[i+1] = doc_name_list[i]\n",
        "print(doc_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jgDHpN3sPnf",
        "outputId": "c2d46c9a-a40c-4cd3-ab1d-7458a685a8e4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'cranfield0025', 2: 'cranfield0015', 3: 'cranfield0021', 4: 'cranfield0022', 5: 'cranfield0016', 6: 'cranfield0013', 7: 'cranfield0023', 8: 'cranfield0014', 9: 'cranfield0017', 10: 'cranfield0024', 11: 'cranfield0012', 12: 'cranfield0019', 13: 'cranfield0020', 14: 'cranfield0018', 15: 'cranfield0011', 16: 'cranfield0006', 17: 'cranfield0009', 18: 'cranfield0002', 19: 'cranfield0008', 20: 'cranfield0004', 21: 'cranfield0007', 22: 'cranfield0005', 23: 'cranfield0003', 24: 'cranfield0010', 25: 'cranfield0001'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc_id in doc_mapping.keys():\n",
        "  doc_name = doc_mapping[doc_id]\n",
        "  path_to_doc = '/content/drive/MyDrive/IR Assigns/toy_ds' + f'/{doc_name}'\n",
        "  f = open(path_to_doc, 'r')\n",
        "  text = ''\n",
        "  for line in f:\n",
        "    for wrd in line.split():\n",
        "      text += ' ' + wrd +' '\n",
        "  f.close()\n",
        "  tokens = word_tokenize(text)\n",
        "  for idx  in range(len(tokens)):\n",
        "    token = tokens[idx]\n",
        "    if token in dictionary:\n",
        "      element = dictionary[token]\n",
        "      if doc_id in element.docs:\n",
        "        element.docs[doc_id].positions.append(idx)\n",
        "      else:\n",
        "        element.docs[doc_id] = doc(doc_id)\n",
        "        document = dictionary[token].docs[doc_id]\n",
        "        document.positions.append(idx)\n",
        "        element.freq= len(dictionary[token].docs)\n",
        "    else:\n",
        "      dictionary[token] = word()\n",
        "      element = dictionary[token]\n",
        "      element.value = token\n",
        "      element.docs[doc_id] = doc(doc_id)\n",
        "      document = element.docs[doc_id]\n",
        "      document.positions.append(idx)\n",
        "print(dictionary['doc'].docs)\n",
        "print(dictionary['doc'].freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jGv5d2njfnU",
        "outputId": "d5d6787a-47c5-4e86-caaf-02bfb0bde8e5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: <__main__.doc object at 0x7f875b1c9550>, 2: <__main__.doc object at 0x7f875b1c9fa0>, 3: <__main__.doc object at 0x7f875b1c95e0>, 4: <__main__.doc object at 0x7f875b1c9dc0>, 5: <__main__.doc object at 0x7f875b205850>, 6: <__main__.doc object at 0x7f875b1dd7f0>, 7: <__main__.doc object at 0x7f875b1ed760>, 8: <__main__.doc object at 0x7f875b284fd0>, 9: <__main__.doc object at 0x7f875b2a3400>, 10: <__main__.doc object at 0x7f875b23f5b0>, 11: <__main__.doc object at 0x7f875b3fcb80>, 12: <__main__.doc object at 0x7f875b1b1c10>, 13: <__main__.doc object at 0x7f875b291bb0>, 14: <__main__.doc object at 0x7f875b1ddcd0>, 15: <__main__.doc object at 0x7f875b1e1970>, 16: <__main__.doc object at 0x7f875b1da760>, 17: <__main__.doc object at 0x7f875b16d310>, 18: <__main__.doc object at 0x7f875b21d850>, 19: <__main__.doc object at 0x7f875b220040>, 20: <__main__.doc object at 0x7f875b20eaf0>, 21: <__main__.doc object at 0x7f875b1ec0d0>, 22: <__main__.doc object at 0x7f875b1f51f0>, 23: <__main__.doc object at 0x7f875b267430>, 24: <__main__.doc object at 0x7f875b26a8b0>, 25: <__main__.doc object at 0x7f875b25e970>}\n",
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZVQzA5ry01G",
        "outputId": "514c4624-31ef-4a9a-f47c-5e578d56ddd5"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[90]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dictionary['doc'].docs[25].positions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlKpfiWhrbq5",
        "outputId": "c85fe820-a389-46c4-cf81-6d26908fbf62"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 98]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5usidBcniGO",
        "outputId": "991e7464-a5db-46c8-e547-7f1b1b32cbd4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['doc', 'docno', '1', 'docno', 'title', 'experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', 'title', 'author', 'brenckman', 'author', 'biblio', 'j', 'ae', 'scs', '25', '1958', '324', 'biblio', 'text', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', 'results', 'intended', 'part', 'evaluation', 'basis', 'different', 'theoretical', 'treatments', 'problem', 'comparative', 'span', 'loading', 'curves', 'together', 'supporting', 'evidence', 'showed', 'substantial', 'part', 'lift', 'increment', 'produced', 'slipstream', 'due', 'destalling', 'boundary', 'layer', 'control', 'effect', 'integrated', 'remaining', 'lift', 'increment', 'subtracting', 'destalling', 'lift', 'found', 'agree', 'well', 'potential', 'flow', 'theory', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'specific', 'configuration', 'experiment', 'text', 'doc']\n"
          ]
        }
      ]
    }
  ]
}